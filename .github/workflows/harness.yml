name: SWHID Testing Harness

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: "17 2 * * *"  # Nightly deep run at 2:17 AM UTC

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-24.04, macos-14, windows-2022]
        python-version: ["3.10", "3.12"]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for commit info
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev] psutil
      
      - name: Run harness unit tests
        run: |
          pytest tests/unit/ tests/integration/ -v --tb=short
      
      - name: Run harness with built-in implementations only
        if: github.event_name != 'schedule'
        run: |
          # Test harness with implementations that are guaranteed to be available
          # (git and pygit2 use Python packages, no external tools needed)
          swhid-harness --impl git,pygit2 --category content --output-format canonical --dashboard-output results.json || true
          echo "Harness test with built-in implementations completed"
      
      - name: Run full test suite (if implementations available)
        if: github.event_name == 'schedule'
        run: |
          # Scheduled runs can try all implementations, but gracefully handle missing ones
          swhid-harness --category content,directory --output-format canonical --dashboard-output results.json || true
          echo "Full test suite completed (some implementations may be skipped)"
      
      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.os }}-py${{ matrix.python-version }}
          path: results.json
          retention-days: 30
      
      - name: Generate test summary
        shell: bash
        run: |
          echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          if [ -f results.json ]; then
            python3 -c "import json; fp=open('results.json'); d=json.load(fp); fp.close(); a=d.get('aggregates',{}); b=a.get('by_implementation',{}); p=sum(s.get('passed',0) for s in b.values()); fa=sum(s.get('failed',0) for s in b.values()); sk=sum(s.get('skipped',0) for s in b.values()); t=p+fa+sk; r=(p/t*100) if t>0 else 0; print(f'| Total Tests | {t} |'); print(f'| Passed | {p} |'); print(f'| Failed | {fa} |'); print(f'| Skipped | {sk} |'); print(f'| Success Rate | {r:.1f}% |') if b else print('| Note | Results format not recognized |')" >> $GITHUB_STEP_SUMMARY 2>&1 || echo "| Note | Error parsing results |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Note | No results file (harness tests may have been skipped) |" >> $GITHUB_STEP_SUMMARY
          fi

  publish-dashboard:
    needs: test
    if: github.ref == 'refs/heads/main' && github.event_name != 'schedule'
    runs-on: ubuntu-24.04
    permissions:
      pages: write
      id-token: write
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.12
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
      
      - name: Merge results
        run: |
          # Create a simple HTML summary if merge_results.py doesn't exist
          if [ -f tools/merge_results.py ]; then
            python tools/merge_results.py artifacts/**/results.json --site site
          else
            mkdir -p site
            echo "<html><body><h1>Test Results</h1><p>Results are available in the artifacts.</p></body></html>" > site/index.html
          fi
      
      - name: Configure Pages
        uses: actions/configure-pages@v5
        continue-on-error: true
      
      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: site
        continue-on-error: true
      
      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4
        continue-on-error: true

  performance-baseline:
    needs: test
    if: github.event_name == 'schedule'
    runs-on: ubuntu-24.04
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts
      
      - name: Update performance baselines
        run: |
          echo "Performance baseline update would go here"
          # This would update baseline performance files
          # and detect regressions
